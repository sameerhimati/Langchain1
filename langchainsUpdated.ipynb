{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Langchains practice making a basic document questioning RAG to chat with our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here are the basic steps\n",
    "1. Document Loading\n",
    "2. Document Splitting\n",
    "3. Vectorstores and Embeddings\n",
    "4. Retrival\n",
    "5. Question Answering\n",
    "6. Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup for os environment variables\n",
    "import os\n",
    "import openai\n",
    "import sys\n",
    "\n",
    "sys.path.append('../..')\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read the local .env file\n",
    "\n",
    "# OpenAI API key\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# Langchain endpoint\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"AmexRag\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.environ[\"LANGSMITH_API_KEY\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Document Loading\n",
    "\n",
    "add the pdf documents from the directory into a place where we can process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import PDF loaders - different loaders to see the difference in output\n",
    "from langchain.document_loaders import PDFPlumberLoader\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loaderpy = PyPDFLoader(\"AmexStatements/2024-02-21.pdf\")\n",
    "loaderplumber = PDFPlumberLoader(\"AmexStatements/2024-02-21.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loaderpy.load() \n",
    "#print(len(pages))\n",
    "#print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = loaderplumber.load()\n",
    "#print(len(pages))\n",
    "#print(pages[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'AmexStatements/2024-02-21.pdf',\n",
       " 'file_path': 'AmexStatements/2024-02-21.pdf',\n",
       " 'page': 0,\n",
       " 'total_pages': 9,\n",
       " 'Subject': '',\n",
       " 'CreationDate': \"D:20240519155457-07'00'\",\n",
       " 'Producer': 'OpenText Output Transformation Engine - 16.3.19                                                     ',\n",
       " 'Author': '',\n",
       " 'Creator': '',\n",
       " 'Title': '',\n",
       " 'ModDate': \"D:20240519155457-07'00'\",\n",
       " 'Keywords': ''}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n",
      "2563\n"
     ]
    }
   ],
   "source": [
    "# Now lets load all the docuemnts into a list\n",
    "loaders = []\n",
    "for file in os.listdir(\"AmexStatements\"):\n",
    "    loaders.append(PyPDFLoader(f\"AmexStatements/{file}\"))\n",
    "# Load PDF\n",
    "\n",
    "docs = []\n",
    "for loader in loaders:\n",
    "    docs.extend(loader.load())\n",
    "\n",
    "print(len(docs))\n",
    "print(len(docs[0].page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Document Splitting\n",
    "\n",
    "we split documents into chunks in order to make it easier for the model to process\n",
    "* **Level 1: Character Splitting** - Simple static character chunks of data\n",
    "* **Level 2: Recursive Character Text Splitting** - Recursive chunking based on a list of separators\n",
    "* **Level 3: Document Specific Splitting** - Various chunking methods for different document types (PDF, Python, Markdown)\n",
    "* **Level 4: Semantic Splitting** - Embedding walk based chunking\n",
    "* **Level 5: Agentic Splitting** - Experimental method of splitting text with an agent-like system. Good for if you believe that token cost will trend to $0.00\n",
    "* **Alternative Representation Chunking + Indexing** - Derivative representations of your raw text that will aid in retrieval and indexing\n",
    "\n",
    "\n",
    "There are many ways of splitting, I highlight 4 most useful for our cases.\n",
    "1. By Characters or Resursively splitting on Characters. - Good Starting place\n",
    "2. By Token. - Good if you have a fixed pricing problem (Not for our case as much)\n",
    "3. By Document Specific Splitting. - Good for specific structures like tables. (Probably most useful for us)\n",
    "4. By Semantic Chuncking. - Good to the understanding of the data (2nd most useful)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter # Doesnt look at whitespace and newlines\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # Good for text but not pdfs\n",
    "from langchain.text_splitter import TokenTextSplitter # Good for price based splitting\n",
    "\n",
    "# text = 'fnsldnfsd'\n",
    "# text_splitter = CharacterTextSplitter(chunk_size = 26, chunk_overlap = 4)\n",
    "# recursive_splitter = RecursiveCharacterTextSplitter(chumk_size = 26, chunk_overlap = 4)\n",
    "# token_splitter = TokenTextSplitter(chunk_size = 6, chunk_overlap = 1)\n",
    "\n",
    "# text_splitter.split_text(text)\n",
    "# recursive_splitter.split_text(text)\n",
    "# token_splitter.split_text(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super Cool below but too many dependencies and isnt working rn due to an Onnx installation issue, moving on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from unstructured.staging.base import elements_to_json\n",
    "# from unstructured.partition.pdf import partition_pdf\n",
    "# import numpy as np\n",
    "# import pdf2image\n",
    "# import wrapt\n",
    "# from pdfminer import psparser\n",
    "\n",
    "# elements = partition_pdf(\n",
    "#     filename=\"AmexStatements/2024-02-21.pdf\",\n",
    "\n",
    "#     # Unstructured Helpers\n",
    "#     strategy=\"hi_res\", \n",
    "#     infer_table_structure=True, \n",
    "#     model_name=\"yolox\"\n",
    "# )\n",
    "\n",
    "# elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "print(len(splits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Vectorstores and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "299\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "persist_directory = 'docs/chroma/'\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=splits,\n",
    "    embedding=OpenAIEmbeddings()\n",
    "    #, persist_directory=persist_directory\n",
    ")\n",
    "print(vectordb._collection.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How much have I spent on Uber? There are various transactions on my statement, give me the sum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vectordb.similarity_search(question,k=3)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem with basic retrival on similarity is we dont get diversity of content if its the same across pages, for that we will use different retrival methods. \n",
    "\n",
    "Examples:\n",
    "1. MMR (Maximum marginal relevance)\n",
    "2. SelfQueryRetriever (The query string to use for vector search)\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Foreign\\nSpend\\n10/13/23 Uber Trip help.uber.com CA\\nSP25Z6LF 60527$18.94\\n10/13/23 CHIPOTLE ONLINE NEWP'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs_mmr = vectordb.max_marginal_relevance_search(question,k=3)\n",
    "docs_mmr[0].page_content[:100] # Based on MMR search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on similarity search on a specific document (metadata)\n",
    "docs = vectordb.similarity_search(\n",
    "    question,\n",
    "    k=3,\n",
    "    filter={\"source\":\"AmexStatements/2024-04-22.pdf\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 6, 'source': 'AmexStatements/2024-04-22.pdf'}\n",
      "{'page': 0, 'source': 'AmexStatements/2024-04-22.pdf'}\n",
      "{'page': 7, 'source': 'AmexStatements/2024-04-22.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Self Query Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "from langchain_openai import OpenAI\n",
    "# Building the Self Query Retriever below few cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The statement the chunk is from, should be one of `AmexStatements/*.pdf` where the * is the date the statement was issued\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"page\",\n",
    "        description=\"The page from the statement\",\n",
    "        type=\"integer\",\n",
    "    ),\n",
    "] \n",
    "\n",
    "# This is the info the retriever will use to construct the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameer/anaconda3/envs/lang/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "document_content_description = \"Bank Statements from American Express for Sameer Himati\"\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct', temperature=0)\n",
    "retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectordb,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n",
    "docs = retriever.get_relevant_documents(question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'page': 5, 'source': 'AmexStatements/2023-10-23.pdf'}\n",
      "{'page': 3, 'source': 'AmexStatements/2024-02-21.pdf'}\n",
      "{'page': 6, 'source': 'AmexStatements/2024-04-22.pdf'}\n",
      "{'page': 13, 'source': 'AmexStatements/2023-11-22.pdf'}\n"
     ]
    }
   ],
   "source": [
    "for d in docs:\n",
    "    print(d.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Compression\n",
    "\n",
    "Another approach for improving the quality of retrieved docs is compression.\n",
    "Information most relevant to a query may be buried in a document with a lot of irrelevant text.\n",
    "Passing that full document through your application can lead to more expensive LLM calls and poorer responses.\n",
    "Contextual compression is meant to fix this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "10/13/23 Uber Trip help.uber.com CA\n",
      "SP25Z6LF 60527$18.94\n",
      "10/14/23 Uber Trip help.uber.com CA\n",
      "SYPLO52W 60523$10.97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "03/15/23 Uber Trip help.uber.com CA\n",
      "HHJZE4HR 33139$28.94\n",
      "03/15/23 Uber Trip help.uber.com CA\n",
      "6NDDBWD5 33122$22.99\n",
      "03/16/23 Uber Trip help.uber.com CA\n",
      "Y7LJAIZZ 60181$6.97\n",
      "03/16/23 Uber Trip help.uber.com CA\n",
      "UCW4EZRF 60148$6.98\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "-$14.07 -$14.07\n",
      "01/31/24 UBER EATS\n",
      "help.uber.com CA\n",
      "KAQ2NFXW 94103-$14.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "Total Fees in2023 $250.00\n",
      "Total Interest in2023 $0.00\n",
      "Days inBilling Period: 31\n",
      "Your Annual Percentage Rate (APR) istheannual interest rate onyour account. Pay Over Time 02/06/2023 27.99 %(v) $0.00 $0.00\n",
      "Cash Advances 02/06/2023 29.99 %(v) $0.00 $0.00p.6/10\n",
      "Amount\n",
      "Amount\n",
      "Total Fees forthis Period $0.00\n",
      "Amount\n",
      "Total Interest Charged forthis Period $0.00\n",
      "Amount\n",
      "Transactions Dated\n",
      "From ToAnnual\n",
      "Percentage\n",
      "RateBalance\n",
      "Subject to\n",
      "Interest RateInterest\n",
      "Charge\n",
      "Total $0.00Fees\n",
      "Interest Charged\n",
      "2023 Fees andInterest Totals Year-to-Date\n",
      "Interest Charge CalculationSHIMATI Account Ending 0-21003\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "def pretty_print_docs(docs):\n",
    "    print(f\"\\n{'-' * 100}\\n\".join([f\"Document {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]))\n",
    "\n",
    "# Wrap our vectorstore\n",
    "llm = OpenAI(temperature=0, model=\"gpt-3.5-turbo-instruct\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "\n",
      "10/13/23 Uber Trip help.uber.com CA\n",
      "SP25Z6LF 60527$18.94\n",
      "10/14/23 Uber Trip help.uber.com CA\n",
      "SYPLO52W 60523$10.97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 2:\n",
      "\n",
      "-$14.07 -$14.07\n",
      "01/31/24 UBER EATS\n",
      "help.uber.com CA\n",
      "KAQ2NFXW 94103-$14.07\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 3:\n",
      "\n",
      "Total Payments and Credits -$3,903.45 -$463.45 -$4,366.90\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Document 4:\n",
      "\n",
      "02/19/23 AplPay STARBUCKS STORE 0835 SAINT PETERSBURG FL\n",
      "FAST FOOD RESTAURANT$3.16\n",
      "02/19/23 AplPay PUBLIX SAINT PETERSBURG FL\n",
      "8636881188$11.27\n",
      "02/19/23 AplPay GALLITO 927540460410837 TAMPA FL\n",
      "TYRODRIGUEZ117@GMAIL.COM$27.82\n",
      "02/20/23 AplPay MIO'S GRILL &CAFE -KAYRA LLC St.Petersburg FL\n",
      "squareup.com/receipts$112.24\n"
     ]
    }
   ],
   "source": [
    "# Combination of MMR search with Compression\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectordb.as_retriever(search_type = \"mmr\")\n",
    ")\n",
    "\n",
    "compressed_docs = compression_retriever.get_relevant_documents(question)\n",
    "pretty_print_docs(compressed_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameer/anaconda3/envs/lang/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' $69.88'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA \n",
    "# Starting a QA chain, lets see if it helps\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever()\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Thanks for asking! Based on the transactions listed on your statement, it appears that you have spent a total of $36.88 on Uber.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hmm the output above is not what we wanted, lets try with a Prompt\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Build prompt\n",
    "template = \"\"\"Use the following pieces of context which is parts of a credit card statement from Sameer Himati to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum. Keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. \n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)\n",
    "\n",
    "# Run chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    return_source_documents=True,\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
    ")\n",
    "result = qa_chain({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nBased on the information provided, it appears that you have spent a total of $34.21 on Uber trips. This includes a $28.94 trip on 03/15/23 and a $6.97 trip on 03/16/23. However, there are also several other transactions on your statement that may be related to Uber, such as a $69.50 charge for VTS Flash Cab and a $56.16 charge for Lyft. Without more specific information, it is difficult to determine the exact amount you have spent on Uber. Additionally, there are some other charges on your statement that may not be related to Uber, such as a $10.00 membership fee for LA Fitness and a $109.20 charge for Keller Williams Experience. It is recommended to review your statement in more detail to accurately determine the total amount spent on Uber. Please note that there may be additional charges for Uber that are not reflected on this statement, such as trailing interest charges. It is important to review all statements and transactions to get an accurate understanding of your total spending on Uber.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hmm so perhaps the problem is in the type of Chain we are using, lets try a different one\n",
    "\n",
    "qa_chain_mr = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectordb.as_retriever(),\n",
    "    chain_type=\"refine\"\n",
    ")\n",
    "result = qa_chain_mr({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' $65.85 ($18.94 + $10.97 + $28.94 + $22.99 + $6.97 + $6.98)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Well better but now im going to try using a Self Query Retriever with the qa chain using map reduce\n",
    "\n",
    "qa_chain_mr_10k = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever= ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    chain_type=\"map_reduce\",\n",
    "    base_retriever=vectordb.as_retriever()\n",
    ")\n",
    ")\n",
    "\n",
    "result = qa_chain_mr_10k({\"query\": question})\n",
    "result[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hmm well after trying all these, looks like we still cant get a good response. I think the problem is to do with the splitting and embedding for cases like this we may need to either retrieve all the docs with info about the question or find an efficient way to search thru the embedding vectors for all instances, so smaller splits perhaps\n",
    "\n",
    "### Lets now do some Evaluation below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sameer/anaconda3/envs/lang/lib/python3.12/site-packages/langchain/chains/llm.py:367: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'query': 'What was the total amount spent on October 16, 2023 at AirCanada by the passenger named ARYAEI/SOHEILA?',\n",
       "  'answer': 'The total amount spent by the passenger named ARYAEI/SOHEILA on October 16, 2023 at AirCanada was $29.98. '},\n",
       " {'query': \"On which date did the passenger named HIMATI/SAMEER MR depart from Chicago O'Hare International Airport on an Etihad Airways flight to Abu Dhabi International Airport?\",\n",
       "  'answer': \"The passenger named HIMATI/SAMEER MR departed from Chicago O'Hare International Airport on February 6th, 2024.\"},\n",
       " {'query': 'What date did the customer make a purchase at Dunkin Donuts in Kenosha, Wisconsin and how much was the charge?',\n",
       "  'answer': 'The customer made a purchase on 04/07/24 at Dunkin Donuts in Kenosha, Wisconsin for $14.08.'},\n",
       " {'query': 'What is the phone number for customer care for American Express cards?',\n",
       "  'answer': 'The phone number for customer care for American Express cards can be found on the first page of your statement or on the back of your card. '}]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "\n",
    "example_gen_chain = QAGenerateChain.from_llm(OpenAI())\n",
    "\n",
    "new_examples = example_gen_chain.apply_and_parse(\n",
    "    [{\"doc\": t} for t in docs[:5]]\n",
    ")\n",
    "\n",
    "examples = []\n",
    "\n",
    "for pair in new_examples:\n",
    "    a = pair.values()\n",
    "    examples.extend(a)\n",
    "examples   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"Foreign\\nSpend\\n10/13/23 Uber Trip help.uber.com CA\\nSP25Z6LF 60527$18.94\\n10/13/23 CHIPOTLE ONLINE NEWPORT BEACH CA\\n1339 92660\\nFAST FOOD RESTAURANT$13.19\\n10/14/23 STATE FARM INSURANCE BLOOMINGTON IL\\n8009566310$318.76\\n10/14/23 PORTILLOS HOT DOGS #10 NAPERVI 1624665 NAPERVILLE IL\\n130187 60540\\nJumbo Hot Dog\\nCelery Salt\\nMustard\\nLCCS$14.87\\n10/14/23 Uber Trip help.uber.com CA\\nSYPLO52W 60523$10.97\\n10/14/23 TST* JIBEK JOLU -LINCOLN 00092818 CHICAGO IL\\nRESTAURANT$63.12\\n10/14/23 CHICK-FIL-A LOMBARD IL\\n6305860830$22.24\\n10/14/23 AplPay 1000 TALES FAMILY RESTAUR 545500001 MTPROSPECT IL\\nINFO@RAINFOODUSA.COM$9.68\\n10/15/23 PARK CHICAGO MOBILE 0539 CHICAGO IL\\n877-242-7901$20.00\\n10/15/23 PETE'S FRESH MARKET OAKBROOK TERRACE IL\\n6308126100$34.14\\n10/15/23 AplPay PETE'S FRESH MARKET OAKBROOK TERRACE IL\\n6308126100$3.96\\n10/16/23 AIRCANADA WINNIPEG\\nAirCanada\\nFrom: To: Carrier: Class:\\nTORONTO LESTER BP CHICAGO O'HARE INT AC X\\nCHICAGO O'HARE INT AC L\\nTicket Number: 0144254108535 Date ofDeparture: 10/18\\nPassenger Name: HIMATHI/KAZIM\\nDocument Type: PASSENGER TICKET$29.98\\n10/16/23 AIRCANADA WINNIPEG\\nAirCanada\\nFrom: To: Carrier: Class:\\nCHICAGO O'HARE INT TORONTO LESTER BP AC X\\nCHICAGO O'HARE INT AC L\\nTicket Number: 0144254108534 Date ofDeparture: 10/18\\nPassenger Name: HIMATHI/KAZIM\\nDocument Type: PASSENGER TICKET$29.98\\n10/16/23 AIRCANADA WINNIPEG\\nAirCanada\\nFrom: To: Carrier: Class:\\nTORONTO LESTER BP CHICAGO O'HARE INT AC X\\nCHICAGO O'HARE INT AC L\\nTicket Number: 0144254108537 Date ofDeparture: 10/18\\nPassenger Name: ARYAEI/SOHEILA\\nDocument Type: PASSENGER TICKET$29.98\\n10/16/23 AIRCANADA WINNIPEG\\nAirCanada\\nFrom: To: Carrier: Class:\\nCHICAGO O'HARE INT TORONTO LESTER BP AC X\\nCHICAGO O'HARE INT AC L\\nTicket Number: 0144254108536 Date ofDeparture: 10/18\\nPassenger Name: ARYAEI/SOHEILA\\nDocument Type: PASSENGER TICKET$29.98p.6/9\\nContinued onnext pageAmountSHIMATI Account Ending 0-21003\\n-denotes Pay Over Time and/or Cash Advance activity\\n40.80\\nCanadian Dollars\\n40.80\\nCanadian Dollars\\n40.80\\nCanadian Dollars\\n40.80\\nCanadian DollarsDetail Continued\", metadata={'page': 5, 'source': 'AmexStatements/2023-10-23.pdf'})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa = qa_chain\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'langchain' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlangchain\u001b[49m\u001b[38;5;241m.\u001b[39mdebug \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'langchain' is not defined"
     ]
    }
   ],
   "source": [
    "langchain.debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = qa.apply(examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation.qa import QAEvalChain\n",
    "\n",
    "eval_chain = QAEvalChain.from_llm(llm)\n",
    "\n",
    "graded_outputs = eval_chain.evaluate(examples, predictions)\n",
    "\n",
    "\n",
    "for i, eg in enumerate(examples):\n",
    "    print(f\"Example {i}:\")\n",
    "    print(\"Question: \" + predictions[i]['query'])\n",
    "    print(\"Real Answer: \" + predictions[i]['answer'])\n",
    "    print(\"Predicted Answer: \" + predictions[i]['result'])\n",
    "    print(\"Predicted Grade: \" + graded_outputs[i]['results'])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Chat!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lang",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
